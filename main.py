# -*- coding: utf-8 -*-
"""Athena_submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Fq15YydsIax5RnJWR2Uh-mbA6vihHHq
"""

# Install the necessary Python libraries
!pip install openai-whisper transformers openai librosa pydub accelerate

# Install ffmpeg (Required for audio processing with Whisper/pydub)
!apt-get install -y ffmpeg

import librosa
import numpy as np
import whisper
from transformers import pipeline
from openai import OpenAI
import getpass

# Paste your key in the box prompted after running this cell
api_key = getpass.getpass("Enter your OpenAI API Key: ")

client = OpenAI(api_key=api_key)

# Initialize clients
sentiment_classifier = pipeline("audio-classification", model="superb/wav2vec2-base-superb-er")

def analyze_audio(audio_path):
    y, sr = librosa.load(audio_path)

    # 1. Pitch & Monotonicity (Standard Deviation of Pitch)
    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
    pitch_std = np.std(pitches[pitches > 0]) # Higher std dev = less monotone

    # 2. Pace & Pauses (Simple energy detection)
    non_silent_intervals = librosa.effects.split(y, top_db=20)
    silence_duration = (len(y) - sum([end-start for start, end in non_silent_intervals])) / sr

    # 3. Emotion Detection (using HuggingFace)
    # Note: Requires 16kHz audio for most models
    emotion_result = sentiment_classifier(audio_path, top_k=1)
    dominant_emotion = emotion_result[0]['label']

    # 4. Heuristic Confidence Score (0-100)
    # Simple logic: penalize for silence and nervousness
    base_score = 80
    if dominant_emotion in ['sad', 'fear']: base_score -= 20
    if pitch_std < 10: base_score -= 10 # Penalize robotic voice

    return {
        "pitch_variation": pitch_std,
        "silence_seconds": silence_duration,
        "dominant_emotion": dominant_emotion,
        "delivery_score": base_score
    }

def analyze_content(audio_path):
    # 1. Transcribe
    model = whisper.load_model("base")
    result = model.transcribe(audio_path)
    transcript = result["text"]

    # 2. LLM Business Evaluation
    system_prompt = """
    You are a venture capital analyst. Analyze the pitch transcript for:
    1. Problem Clarity
    2. Revenue Model
    3. Market Opportunity
    4. Competitive Advantage
    Output a JSON with scores (0-100) for each and a brief critique.
    """

    response = client.chat.completions.create(
        model="gpt-5-nano",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": transcript}
        ]
    )

    return transcript, response.choices[0].message.content

def generate_shark_feedback(transcript, audio_metrics, content_analysis):

    personas = {
        "The Visionary": "Focus on the big picture, future potential, and innovation. Ignore small financial details.",
        "The Skeptic": "Look for holes in the plan. Doubt the valuation. Question the competition.",
        "The Finance Shark": "Care only about margins, customer acquisition costs, and path to profitability."
    }

    feedback_report = {}

    context = f"""
    Pitch Transcript: {transcript}
    Audio Delivery Analysis: The speaker was {audio_metrics['dominant_emotion']} with a delivery score of {audio_metrics['delivery_score']}.
    Business Analysis: {content_analysis}
    """

    for shark_name, shark_style in personas.items():
        prompt = f"You are {shark_name}. Your style is: {shark_style}. Give a 2-sentence critique and a final 'Invest/No Invest' decision based on the context provided."

        response = client.chat.completions.create(
            model="gpt-5-nano",
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": context}
            ]
        )
        feedback_report[shark_name] = response.choices[0].message.content

    return feedback_report

# Main Execution
audio_file = "user_pitch2.mp3"

print("--- Pipeline 1: Analyzing Voice ---")
audio_data = analyze_audio(audio_file)
print(f"Detected Emotion: {audio_data['dominant_emotion']}")

print("--- Pipeline 2: Analyzing Content ---")
transcript, business_score = analyze_content(audio_file)

print("--- Pipeline 3: Shark Tank Panel ---")
final_feedback = generate_shark_feedback(transcript, audio_data, business_score)

# Display Output
for shark, feedback in final_feedback.items():
    print(f"\nðŸ¦ˆ {shark}: {feedback}")

